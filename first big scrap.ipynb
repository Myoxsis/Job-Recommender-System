{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config\n",
    "\n",
    "# Create Backup\n",
    "# --------------------\n",
    "\n",
    "os.system(\"mkdir backup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "#    []   scrapRichemont()                     #    []   scrapRichemont()\n",
    "#    []   scrapDassaultAviation()              #    []   scrapRichemont()\n",
    "#    []   scrapAirFrance()                     #    []   scrapRichemont()\n",
    "#    []   scrapSanofi()                        #    []   scrapRichemont()\n",
    "#    []   scrapHermes()                        #    []   scrapRichemont()\n",
    "#    []   scrapFramatome()                     #    []   scrapRichemont()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize function\n",
    "def scrapRichemont():\n",
    "    richemont_job_db = []\n",
    "\n",
    "    browser = webdriver.Chrome('/Users/maxime_alain/Downloads/chromedriver')\n",
    "    browser.get('https://jobs.richemont.com/search/?q=&sortColumn=referencedate&sortDirection=desc&optionsFacetsDD_country=FR&startrow=1')\n",
    "\n",
    "    soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "\n",
    "    for x in soup.find_all('tr', attrs={'class' : 'data-row clickable'}):\n",
    "        row_scrap = []\n",
    "        job_title = x.find('span', attrs={'class' : 'jobTitle'}).text.replace('\\n', '')\n",
    "        job_link = x.find('span', attrs={'class' : 'jobTitle'}).find('a')['href']\n",
    "        company = x.find('span', attrs={'class' : 'jobFacility'}).text.replace('\\n', '')\n",
    "        departement = x.find('span', attrs={'class' : 'jobDepartment'}).text.replace('\\n', '')\n",
    "        location = x.find('span', attrs={'class' : 'jobLocation'}).text.replace('\\n', '').strip()\n",
    "\n",
    "        row_scrap.append(job_title)\n",
    "        row_scrap.append(job_link)\n",
    "        row_scrap.append(company)\n",
    "        row_scrap.append(departement)\n",
    "        row_scrap.append(location)\n",
    "\n",
    "        richemont_job_db.append(row_scrap)\n",
    "\n",
    "    i = 14\n",
    "    while i <= int(soup.find('a', class_='paginationItemLast')['href'][-3:]) :\n",
    "        browser.get(f'https://jobs.richemont.com/search/?q=&sortColumn=referencedate&sortDirection=desc&optionsFacetsDD_country=FR&startrow={i}')\n",
    "\n",
    "        soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "\n",
    "        for x in soup.find_all('tr', attrs={'class' : 'data-row clickable'}):\n",
    "            row_scrap = []\n",
    "            job_title = x.find('span', attrs={'class' : 'jobTitle'}).text.replace('\\n', '')\n",
    "            job_link = x.find('span', attrs={'class' : 'jobTitle'}).find('a')['href']\n",
    "            company = x.find('span', attrs={'class' : 'jobFacility'}).text.replace('\\n', '')\n",
    "            departement = x.find('span', attrs={'class' : 'jobDepartment'}).text.replace('\\n', '')\n",
    "            location = x.find('span', attrs={'class' : 'jobLocation'}).text.replace('\\n', '').strip()\n",
    "\n",
    "            row_scrap.append(job_title)\n",
    "            row_scrap.append(job_link)\n",
    "            row_scrap.append(company)\n",
    "            row_scrap.append(departement)\n",
    "            row_scrap.append(location)\n",
    "\n",
    "            richemont_job_db.append(row_scrap)\n",
    "\n",
    "        i = i + 14\n",
    "\n",
    "    richemont_job_db = pd.DataFrame(richemont_job_db)\n",
    "    richemont_job_db = richemont_job_db.rename(columns={0 : \"Position\", 1 : \"link\", 2 : \"Company\",\n",
    "                                                        3 : \"Department\", 4 : \"Location\"})\n",
    "\n",
    "    richemont_job_db_copy = richemont_job_db\n",
    "    richemont_job_db_copy['desc'] = ''\n",
    "\n",
    "    # Add Desc to job offer\n",
    "    # add https://jobs.richemont.com before links to connect\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i <= len(richemont_job_db_copy)-1:\n",
    "        browser.get('https://jobs.richemont.com' + richemont_job_db_copy['link'][i])\n",
    "        soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "        richemont_job_db_copy['desc'][i] = soup.find('span', class_=\"jobdescription\").text\n",
    "        i = i + 1\n",
    "\n",
    "    richemont_job_db_copy.to_csv('backup/richemont_db.csv')\n",
    "    richemont_job_db_copy.to_json('backup/richemont_db.json')\n",
    "    \n",
    "    richemont_job_db_copy.head()\n",
    "    \n",
    "def scrapDassaultAviation():\n",
    "    dassaultAviation_db = []\n",
    "\n",
    "    browser = webdriver.Chrome('/Users/maxime_alain/Downloads/chromedriver')\n",
    "\n",
    "    i = 1\n",
    "    while i <= 10:\n",
    "        browser.get(f'https://carriere.dassault-aviation.com/offre-de-emploi/liste-offres.aspx?page={i}&LCID=1036')\n",
    "        soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "\n",
    "        for x in soup.find_all('li', class_='ts-offer-list-item'):\n",
    "                row_scrap = []\n",
    "                job_title = x.find('a', attrs={'class' : 'ts-offer-list-item__title-link'}).text.replace('\\n', '').strip()\n",
    "                job_link = x.find('a', attrs={'class' : 'ts-offer-list-item__title-link'})['href']\n",
    "                company = \"Dassault Aviation\"\n",
    "                departement = 'N/A'\n",
    "                details = x.find('ul', attrs={'class' : 'ts-offer-list-item__description'}).find_all('li')\n",
    "\n",
    "                row_scrap.append(job_title)\n",
    "                row_scrap.append(job_link)\n",
    "                row_scrap.append(company)\n",
    "                row_scrap.append(departement)\n",
    "                row_scrap.append(details)\n",
    "\n",
    "                dassaultAviation_db.append(row_scrap)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    dassaultAviation_db = pd.DataFrame(dassaultAviation_db)\n",
    "    dassaultAviation_db = dassaultAviation_db.rename(columns={0 : \"Position\", 1 : \"link\", 2 : \"Company\", 3 : \"Department\", 4 : \"Location\"}) #\n",
    "    dassaultAviation_db_copy = dassaultAviation_db\n",
    "    dassaultAviation_db_copy['desc'] = ''\n",
    "\n",
    "    # Add Desc to job offer\n",
    "    # add https://carriere.dassault-aviation.com before links to connect\n",
    "\n",
    "    i = 0\n",
    "    while i <= len(dassaultAviation_db_copy)-1:\n",
    "        browser.get('https://carriere.dassault-aviation.com' + dassaultAviation_db_copy['link'][i])\n",
    "        soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "        dassaultAviation_db_copy['desc'][i] = soup.find('div', attrs={'id' : 'contenu-ficheoffre'}).text\n",
    "        i = i + 1\n",
    "\n",
    "    dassaultAviation_db_copy.to_csv('backup/dassaultAviation_db.csv')\n",
    "    dassaultAviation_db_copy.head()\n",
    "    \n",
    "def scrapAirFrance():\n",
    "    browser = webdriver.Chrome('/Users/maxime_alain/Downloads/chromedriver')\n",
    "\n",
    "    airFrance_db = []\n",
    "\n",
    "    browser.get('https://recrutement.airfrance.com/offre-de-emploi/liste-offres.aspx')\n",
    "    soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "\n",
    "    for x in soup.find_all('li', class_='ts-offer-list-item'):\n",
    "                row_scrap = []\n",
    "                job_title = x.find('a', attrs={'class' : 'ts-offer-list-item__title-link'}).text.replace('\\n', '').strip()\n",
    "                job_link = x.find('a', attrs={'class' : 'ts-offer-list-item__title-link'})['href']\n",
    "                company = \"Air France\"\n",
    "                departement = 'N/A'\n",
    "                details = x.find('ul', attrs={'class' : 'ts-offer-list-item__description'}).find_all('li')\n",
    "\n",
    "                row_scrap.append(job_title)\n",
    "                row_scrap.append(job_link)\n",
    "                row_scrap.append(company)\n",
    "                row_scrap.append(departement)\n",
    "                row_scrap.append(details)\n",
    "\n",
    "                airFrance_db.append(row_scrap)\n",
    "\n",
    "    airFrance_db = pd.DataFrame(airFrance_db)\n",
    "    airFrance_db = airFrance_db.rename(columns={0 : \"Position\", 1 : \"link\", 2 : \"Company\", 3 : \"Department\", 4 : \"Location\"}) #\n",
    "    airFrance_db_copy = airFrance_db\n",
    "    airFrance_db_copy['desc'] = ''\n",
    "\n",
    "    i = 0\n",
    "    while i <= len(airFrance_db_copy)-1:\n",
    "        browser.get('https://recrutement.airfrance.com' + airFrance_db_copy['link'][i])\n",
    "        soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "        airFrance_db_copy['desc'][i] = soup.find('div', attrs={'id' : 'contenu-ficheoffre'}).text\n",
    "        i = i + 1\n",
    "\n",
    "    airFrance_db_copy.to_csv('backup/airFrance_db.csv')\n",
    "    airFrance_db_copy.head()\n",
    "\n",
    "def scrapSanofi():\n",
    "    browser = webdriver.Chrome('/Users/maxime_alain/Downloads/chromedriver')\n",
    "    browser.get('https://fr.jobs.sanofi.com/recherche-d%27offres/?p=1')\n",
    "    soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "\n",
    "    sanofi_db = []\n",
    "\n",
    "    i = 1\n",
    "    while i <= int(soup.find('input', class_='pagination-current')['max']):\n",
    "        browser.get(f'https://fr.jobs.sanofi.com/recherche-d%27offres/?p={i}')\n",
    "        soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "        for x in soup.find('ul', class_='unstyled-list job-list').find_all('li'):\n",
    "                    row_scrap = []\n",
    "                    job_title = x.find('h2', attrs={'class' : 'h3-style'}).text.replace('\\n', '').strip()\n",
    "                    job_link = x.find('a')['href']\n",
    "                    company = \"Sanofi\"\n",
    "                    departement = 'N/A'\n",
    "                    details = x.find('span', attrs={'class' : 'job-location'}).text\n",
    "\n",
    "                    row_scrap.append(job_title)\n",
    "                    row_scrap.append(job_link)\n",
    "                    row_scrap.append(company)\n",
    "                    row_scrap.append(departement)\n",
    "                    row_scrap.append(details)\n",
    "\n",
    "                    sanofi_db.append(row_scrap)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    sanofi_db = pd.DataFrame(sanofi_db)\n",
    "    sanofi_db = sanofi_db.rename(columns={0 : \"Position\", 1 : \"link\", 2 : \"Company\", 3 : \"Department\", 4 : \"Location\"}) #\n",
    "    sanofi_db_copy = sanofi_db\n",
    "    sanofi_db_copy['desc'] = ''\n",
    "\n",
    "    i = 0\n",
    "    while i <= len(sanofi_db_copy)-1:\n",
    "        browser.get('https://fr.jobs.sanofi.com/' + sanofi_db_copy['link'][i])\n",
    "        soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "        try :\n",
    "            sanofi_db_copy['desc'][i] = soup.find('div', attrs={'class' : 'ats-description'}).text\n",
    "        except :\n",
    "            sanofi_db_copy['desc'][i] = ''\n",
    "        i = i + 1\n",
    "\n",
    "    sanofi_db_copy.to_csv('backup/sanofi_db.csv')\n",
    "    sanofi_db_copy.head()\n",
    "\n",
    "def scrapHermes():\n",
    "    browser = webdriver.Chrome('/Users/maxime_alain/Downloads/chromedriver')\n",
    "\n",
    "    browser.get('https://talents.hermes.com/fr/')\n",
    "    soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "\n",
    "    hermes_db = []\n",
    "\n",
    "    for x in soup.find_all('div', class_='card-block'):\n",
    "                    row_scrap = []\n",
    "                    job_title = x.find('h2', attrs={'class' : 'offer-description bd-highlight block-with-text'}).text.replace('\\n', '').strip()\n",
    "                    job_link = x.find('a')['href']\n",
    "                    company = \"Hermes\"\n",
    "                    departement = 'N/A'\n",
    "                    details = []\n",
    "                    for v in x.find('ul', attrs={'class' : 'offer-list_info'}).find_all('li'): \n",
    "                        item = v.text\n",
    "                        details.append(item)\n",
    "\n",
    "                    #details.append(details_item)\n",
    "\n",
    "                    row_scrap.append(job_title)\n",
    "                    row_scrap.append(job_link)\n",
    "                    row_scrap.append(company)\n",
    "                    row_scrap.append(departement)\n",
    "                    row_scrap.append(details)\n",
    "\n",
    "                    hermes_db.append(row_scrap)\n",
    "\n",
    "    hermes_db = pd.DataFrame(hermes_db)\n",
    "    hermes_db = hermes_db.rename(columns={0 : \"Position\", 1 : \"link\", 2 : \"Company\", 3 : \"Department\", 4 : \"Location\"}) #\n",
    "    hermes_db_copy = hermes_db\n",
    "    hermes_db_copy['desc'] = ''\n",
    "\n",
    "    i = 0\n",
    "    while i <= len(hermes_db_copy)-1:\n",
    "        browser.get('https://talents.hermes.com/' + hermes_db_copy['link'][i])\n",
    "        soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "        try :\n",
    "            hermes_db_copy['desc'][i] = soup.find('div', attrs={'id' : 'detailOffer'}).text\n",
    "        except :\n",
    "            hermes_db_copy['desc'][i] = ''\n",
    "        i = i + 1\n",
    "\n",
    "    hermes_db_copy.to_csv('backup/hermes_db.csv')\n",
    "    hermes_db_copy.head()\n",
    "    \n",
    "def scrapFramatome():\n",
    "    browser = webdriver.Chrome('/Users/maxime_alain/Downloads/chromedriver')\n",
    "    framatome_db = []\n",
    "\n",
    "    i = 1\n",
    "    while i <= 44:\n",
    "        browser.get(f'https://framatome-career.talent-soft.com/offre-de-emploi/liste-offres.aspx?page={i}&LCID=1033')\n",
    "        soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "\n",
    "        for x in soup.find_all('div', class_='ts-offer-card Layer'):\n",
    "                        row_scrap = []\n",
    "                        job_title = x.find('h3', attrs={'class' : 'ts-offer-card__title'}).text.replace('\\n', '').strip()\n",
    "                        job_link = x.find('h3', attrs={'class' : 'ts-offer-card__title'}).find('a')['href']\n",
    "                        company = \"Framatome\"\n",
    "                        departement = 'N/A'\n",
    "                        details = []\n",
    "                        for v in x.find('ul', attrs={'class' : 'ts-offer-card-content__list'}).find_all('li'): \n",
    "                            item = v.text\n",
    "                            details.append(item)\n",
    "\n",
    "                        row_scrap.append(job_title)\n",
    "                        row_scrap.append(job_link)\n",
    "                        row_scrap.append(company)\n",
    "                        row_scrap.append(departement)\n",
    "                        row_scrap.append(details)\n",
    "\n",
    "                        framatome_db.append(row_scrap)\n",
    "        i = i + 1\n",
    "\n",
    "    framatome_db = pd.DataFrame(framatome_db)\n",
    "    framatome_db = framatome_db.rename(columns={0 : \"Position\", 1 : \"link\", 2 : \"Company\", 3 : \"Department\", 4 : \"Location\"}) #\n",
    "    framatome_db_copy = framatome_db\n",
    "    framatome_db_copy['desc'] = ''\n",
    "\n",
    "    i = 0\n",
    "    while i <= len(framatome_db_copy)-1:\n",
    "        browser.get('https://framatome-career.talent-soft.com/' + framatome_db_copy['link'][i])\n",
    "        soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "        try :\n",
    "            framatome_db_copy['desc'][i] = soup.find('div', attrs={'class' : 'ts-offer-page__content'}).text\n",
    "        except :\n",
    "            framatome_db_copy['desc'][i] = ''\n",
    "        i = i + 1\n",
    "\n",
    "    framatome_db_copy.to_csv('backup/framatome_db.csv')\n",
    "    framatome_db_copy.head()\n",
    "    \n",
    "def scrapEngie():\n",
    "    browser = webdriver.Chrome('/Users/maxime_alain/Downloads/chromedriver')\n",
    "\n",
    "    engie_db = []\n",
    "\n",
    "    i = 1\n",
    "    while i <= 315:\n",
    "        browser.get(f'https://jobs.engie.com/jobs/search/76073636/page{i}')\n",
    "        soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "        for x in soup.find_all('div', class_='jlr_right_hldr'):\n",
    "            row_scrap = []\n",
    "            job_title = x.find('p').text.replace('\\n', '').strip()\n",
    "            job_link = x.find('a')['href']\n",
    "            company = \"Engie\"\n",
    "            departement = x.find('p', attrs={'class' : 'jlr_company'}).text.replace('\\n', '').strip()\n",
    "            details = []\n",
    "            for v in x.find_all('p', attrs={'class' : 'jlr_cat_loc'}): \n",
    "                item = v.text.replace('\\t', '').replace('\\n', '')\n",
    "                details.append(item)\n",
    "\n",
    "            row_scrap.append(job_title)\n",
    "            row_scrap.append(job_link)\n",
    "            row_scrap.append(company)\n",
    "            row_scrap.append(departement)\n",
    "            row_scrap.append(details)\n",
    "\n",
    "            engie_db.append(row_scrap)\n",
    "        i = i + 1\n",
    "\n",
    "\n",
    "    engie_db = pd.DataFrame(engie_db)\n",
    "    engie_db = engie_db.rename(columns={0 : \"Position\", 1 : \"link\", 2 : \"Company\", 3 : \"Department\", 4 : \"Location\"}) #\n",
    "    engie_db_copy = engie_db\n",
    "    engie_db_copy['desc'] = ''\n",
    "\n",
    "    i = 0\n",
    "    while i <= len(engie_db_copy)-1:\n",
    "        browser.get(engie_db_copy['link'][i])\n",
    "        soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "        try :\n",
    "            engie_db_copy['desc'][i] = soup.find('div', attrs={'id' : 'description_box'}).text\n",
    "        except :\n",
    "            engie_db_copy['desc'][i] = ''\n",
    "        i = i + 1\n",
    "\n",
    "    engie_db_copy.to_csv('backup/engie_db.csv')\n",
    "    engie_db_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Air Liquide Scrap\n",
    "# ---------------\n",
    "#    - Issue : redirect to homepage instead of career page\n",
    "\n",
    "#browser.get('https://www.airliquide.com/fr/carrieres/offres-emploi')\n",
    "#soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "#airLiquide_db = []\n",
    "#for x in soup.find_all('tr'):\n",
    "#    row_scrap = []\n",
    "#    job_title = x.find('a').text.replace('\\n', '').strip()\n",
    "#    job_link = x.find('h3', attrs={'class' : 'ts-offer-card__title'}).find('a')['href']\n",
    "#    company = \"Framatome\"\n",
    "#    departement = 'N/A'\n",
    "#    details = []\n",
    "#    for v in x.find('ul', attrs={'class' : 'ts-offer-card-content__list'}).find_all('li'): \n",
    "#        item = v.text\n",
    "#        details.append(item)\n",
    "\n",
    "#    row_scrap.append(job_title)\n",
    "#    row_scrap.append(job_link)\n",
    "#    row_scrap.append(company)\n",
    "#    row_scrap.append(departement)\n",
    "#    row_scrap.append(details)\n",
    "\n",
    "#    airLiquide_db.append(row_scrap)\n",
    "\n",
    "#airLiquide_db = pd.DataFrame(airLiquide_db)\n",
    "#airLiquide_db = airLiquide_db.rename(columns={0 : \"Position\", 1 : \"link\", 2 : \"Company\", 3 : \"Department\", 4 : \"Location\"}) #\n",
    "#airLiquide_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.airliquide.com/fr/carrieres/offres-emploi\n",
    "https://jobsearch.alstom.com/search/?createNewAlert=false&q=&locationsearch=\n",
    "https://cc.wd3.myworkdayjobs.com/fr-FR/ChanelCareers\n",
    "https://jobs.danone.com/search/?createNewAlert=false&q=&locationsearch=&optionsFacetsDD_country=FR&optionsFacetsDD_facility=&optionsFacetsDD_department=Experienced+professionals\n",
    "https://careers.3ds.com/jobs?woc=%7B%22country%22%3A%5B%22country%2Ffrance%22%5D%7D&wocset=6\n",
    "http://careers.disneylandparis.com/en/management-business/supply-chain-procurement\n",
    "https://www.edf.fr/edf-recrute\n",
    "https://jobs.engie.com\n",
    "https://recrutement.fnacdarty.com/accueil.aspx?LCID=1036\n",
    "https://www.place-emploi-public.gouv.fr\n",
    "https://jobs.gecareers.com/global/en/search-results\n",
    "https://www.invivo-group.com/fr/nos-offres\n",
    "https://kering.wd3.myworkdayjobs.com/fr-FR/Kering?source=LinkedIn_Slots\n",
    "https://careers.loreal.com/en_US/jobs/SearchJobs/\n",
    "https://www.lisi-aerospace.com/en/join-us/careers/\n",
    "https://www.lvmh.fr/talents/nous-rejoindre/nos-offres/liste-des-offres/?job=&place=&experience=&activity=&contract=&reference=#gt_offers-results\n",
    "https://www.mbda-systems.com/jobs/?gestmax%5Bvac_sector%5D=&gestmax%5Bvac_localisation%5D=001&gestmax%5Bvac_job_type%5D=\n",
    "https://jobs.moncler.com/search/?createNewAlert=false&q=&locationsearch=\n",
    "https://motul-recrute.talent-soft.com/job/list-of-jobs.aspx\n",
    "https://www.naval-group.com/fr/nous-rejoindre-85?keywords=&offerFamilyCategory=&contractType=&country=&city=&op=Rechercher&form_build_id=form-NzrJ7AsvKwvEKPwLDA2P5Qu6e3T6EKUpkTDoHGZS6Is&form_id=talent_soft_offers_filters_form#offer-list-content\n",
    "https://nexter-recrutement.profils.org/accueil.aspx?LCID=1036\n",
    "https://orange.jobs/jobs/search.do?keyword=\n",
    "https://pernodricard.wd3.myworkdayjobs.com/fr-FR/pernod-ricard\n",
    "https://jobs.groupe-psa.com/offre-de-emploi/liste-offres.aspx?mode=layer&lcid=1036&facet_JobDescription_Contract=577\n",
    "https://renault.referrals.selectminds.com/\n",
    "https://www.safran-group.com/jobs\n",
    "https://joinus.saint-gobain.com/fr\n",
    "https://www.sodern.com/website/fr/ref/CarriÃ¨res_262.html\n",
    "https://hris-suez.csod.com/ats/careersite/search.aspx?site=8&c=hris-suez&sid=%5e%5e%5eHJe5gko1mldbDMyZ8oI9Lw%3d%3d\n",
    "https://careers.hr.technipfmc.com\n",
    "https://emploi.thalesgroup.com/recherche-d%27offres\n",
    "https://krb-sjobs.brassring.com/TGnewUI/Search/Home/Home?partnerid=30080&siteid=6559#home\n",
    "https://career012.successfactors.eu/career?company=VALLOUREC&site=VjItcmY2YVFFcnJMYWhIb3RmMzhTYU9Ldz09\n",
    "https://emplois.vinci.com/recherche-d%27offres\n",
    "https://www.nestle.fr/jobs/search-jobs?keyword=&country=FR&location=&career_area=All&company=All\n",
    "https://www.smcp.com/fr/talents/offres-d-emploi/?keywords=&geographicalLocation=22&offerCountry=79&offerRegion=&organisation=&offerFamilyCategory=&contractType=&experienceLevel=\n",
    "https://pfizer.wd1.myworkdayjobs.com/PfizerCareers/5/refreshFacet/318c8bb6f553100021d223d9780d30be\n",
    "https://careers.faurecia.com/search/?createNewAlert=false&q=&locationsearch=france&optionsFacetsDD_customfield3=&optionsFacetsDD_country=&optionsFacetsDD_shifttype=Unlimited\n",
    "https://www.emploi.sncf.com/nos-offres/contrat/577-578/localisation/40629/\n",
    "https://careers.ratpdev.com/offre-de-emploi/liste-offres.aspx?page=3&LCID=1036\n",
    "https://arianegroup.wd3.myworkdayjobs.com/EXTERNALALL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
